{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIg2pgwZHoKNJAq1cbWKi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeffBla/implement_transformer_from_scratch/blob/main/implement_transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "P6ugu88GWqb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadSelfAttention(nn.Module):\n",
        "  def __init__(self, k, head=4, mask=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "    self.mask = mask\n",
        "\n",
        "    self.to_query = nn.Linear(k, k, bias=False)\n",
        "    self.to_key = nn.Linear(k, k, bias=False)\n",
        "    self.to_value = nn.Linear(k, k, bias=False)\n",
        "\n",
        "    self.unitify_layer = nn.Linear(k, k)\n",
        "\n",
        "  def forward(self, x, kv=None):\n",
        "    \"\"\" x : (batch_size × seq_length × embed_dim) \"\"\"\n",
        "\n",
        "    h = self.head\n",
        "    if kv is None:\n",
        "      kv = x\n",
        "    b,tq,k = x.size()\n",
        "    _,tk,_ = kv.size()\n",
        "\n",
        "    s = k // h\n",
        "\n",
        "    q = self.to_query(x).view(b,tq,h,s)\n",
        "    k_ = self.to_key(kv).view(b,tk,h,s)\n",
        "    v = self.to_value(kv).view(b,tk,h,s)\n",
        "\n",
        "    attn_scores = torch.einsum(\"bths,behs->bhte\", (q,k_)) / k**0.5\n",
        "\n",
        "    if self.mask:\n",
        "      indices = torch.triu_indices(tq, tk, offset=1)\n",
        "      attn_scores[: , indice[0], indice[1]] = float(\"-inf\")\n",
        "\n",
        "    attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    out = torch.einsum(\"bhtd,bdhe->bthe\", (attn_scores,v))\n",
        "    out = out.reshape((b,tq,k))\n",
        "\n",
        "    return self.unitify_layer(out)"
      ],
      "metadata": {
        "id": "O9i86WbsYnJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, head):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "\n",
        "    self.attention = MultiheadSelfAttention(k, head)\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(k, 4*k),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*k, k)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attended = self.attention(x)\n",
        "    norm1ed = self.norm1(attended + x)\n",
        "    ff_out = self.ff(norm1ed)\n",
        "\n",
        "    return self.norm2(ff_out+norm1ed)"
      ],
      "metadata": {
        "id": "RhsP0kwynoRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformer(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.pos_emb = nn.Embedding(seq_len, k)\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    token_emb = self.token_emb(x)\n",
        "    b, t, k = token_emb.size()\n",
        "\n",
        "    pos_emb = torch.arange(t)\n",
        "    pos_emb = self.pos_emb(pos_emb).unsqueeze(0).expand([b,t,k])\n",
        "\n",
        "    emb = token_emb + pos_emb\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    out = self.ff(out.mean(dim=1))\n",
        "\n",
        "    return F.log_softmax(out, dim=1)"
      ],
      "metadata": {
        "id": "TKHL7Ap0-7O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformerCLS(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.pos_emb = nn.Embedding(seq_len+1, k)\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,k))\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b,t = x.size()\n",
        "    cls_token = self.cls_token.expand([b,1,self.k])\n",
        "    token_emb = self.token_emb(x)\n",
        "    token_emb = torch.cat((cls_token, token_emb), dim=1)\n",
        "\n",
        "    pos_emb = torch.arange(t+1, device=x.device)\n",
        "    pos_emb = self.pos_emb(pos_emb).unsqueeze(0).expand([b,t+1,self.k])\n",
        "\n",
        "    emb = token_emb + pos_emb\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    out = out[:,0]\n",
        "\n",
        "    return self.ff(out)"
      ],
      "metadata": {
        "id": "RPD7Xaq0AV_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTTransformer(nn.Module):\n",
        "  def __init__(self, k, head, depth, patch_size, img_size, channel, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.patch_dim = pathch_size * patch_size * channel\n",
        "    self.num_patch = (img_size // patch_size) ** 2\n",
        "    self.token_emb = nn.Linear(patch_dim, k)\n",
        "    self.pos_emb = nn.Embedding(num_patch+1, k)\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,k))\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.size()\n",
        "    p = self.pathch_size\n",
        "\n",
        "    x = rearrange(x, \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\",\n",
        "            p1=p, p2=p)\n",
        "\n",
        "    token_emb = self.token_emb(x)\n",
        "    b, t, k = token_emb.size()\n",
        "\n",
        "    pos_emb = torch.arange(t)\n",
        "    pos_emb = self.pos_emb(pos_emb).unsqueeze(0).expand([b,t,k])\n",
        "\n",
        "    emb = token_emb + pos_emb\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    out = self.ff(out.mean(dim=1))\n",
        "\n",
        "    return F.log_softmax(out, dim=1)"
      ],
      "metadata": {
        "id": "3DwaZ-pHvisf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}