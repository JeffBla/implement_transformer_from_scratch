{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu3zvkexUaUVDBh0oDacDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeffBla/implement_transformer_from_scratch/blob/main/implement_transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "P6ugu88GWqb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadSelfAttention(nn.Module):\n",
        "  def __init__(self, k, head=4, mask=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "    self.mask = mask\n",
        "\n",
        "    self.to_query = nn.Linear(k, k, bias=False)\n",
        "    self.to_key = nn.Linear(k, k, bias=False)\n",
        "    self.to_value = nn.Linear(k, k, bias=False)\n",
        "\n",
        "    self.unitify_layer = nn.Linear(k, k)\n",
        "\n",
        "  def forward(self, x, kv=None):\n",
        "    \"\"\" x : (batch_size × seq_length × embed_dim) \"\"\"\n",
        "\n",
        "    h = self.head\n",
        "    if kv is None:\n",
        "      kv = x\n",
        "    b,tq,k = x.size()\n",
        "    _,tk,_ = kv.size()\n",
        "\n",
        "    s = k // h\n",
        "\n",
        "    q = self.to_query(x).view(b,tq,h,s)\n",
        "    k_ = self.to_key(kv).view(b,tk,h,s)\n",
        "    v = self.to_value(kv).view(b,tk,h,s)\n",
        "\n",
        "    attn_scores = torch.einsum(\"bths,behs->bhte\", (q,k_)) / k**0.5\n",
        "\n",
        "    if self.mask:\n",
        "      mask = torch.triu(torch.ones(tq,tk, device=x.device), diagonal=1).bool()\n",
        "      attn_scores = attn_scores.masked_fill(mask[None, None, :, :],\n",
        "                          float('-inf'))\n",
        "\n",
        "    attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    out = torch.einsum(\"bhtd,bdhe->bthe\", (attn_scores,v))\n",
        "    out = out.reshape((b,tq,k))\n",
        "\n",
        "    return self.unitify_layer(out)"
      ],
      "metadata": {
        "id": "O9i86WbsYnJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, head, is_cross_attention=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "    self.is_cross_attention = is_cross_attention\n",
        "\n",
        "    self.self_attn = MultiheadSelfAttention(k, head)\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    if is_cross_attention:\n",
        "      self.cross_attn = MultiheadSelfAttention(k, head)\n",
        "      self.cross_norm = nn.LayerNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(k, 4*k),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*k, k)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "  def forward(self, x, enc_out=None):\n",
        "    attended = self.self_attn(x)\n",
        "    norm1ed = self.norm1(attended + x)\n",
        "    x = norm1ed\n",
        "\n",
        "    if self.is_cross_attention and enc_out is not None:\n",
        "      cross_attended = self.cross_attn(x, enc_out)\n",
        "      cross_normed = self.cross_norm(cross_attended + x)\n",
        "      x = cross_normed\n",
        "\n",
        "    ff_out = self.ff(x)\n",
        "\n",
        "    return self.norm2(ff_out + x)"
      ],
      "metadata": {
        "id": "RhsP0kwynoRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformer(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.pos_emb = nn.Embedding(seq_len, k)\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    token_emb = self.token_emb(x)\n",
        "    b, t, k = token_emb.size()\n",
        "\n",
        "    pos_emb = torch.arange(t)\n",
        "    pos_emb = self.pos_emb(pos_emb).unsqueeze(0).expand([b,t,k])\n",
        "\n",
        "    emb = token_emb + pos_emb\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    out = self.ff(out.mean(dim=1))\n",
        "\n",
        "    return F.log_softmax(out, dim=1)"
      ],
      "metadata": {
        "id": "TKHL7Ap0-7O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformerCLS(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.pos_emb = nn.Embedding(seq_len+1, k)\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,k))\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b,t = x.size()\n",
        "    cls_token = self.cls_token.expand([b,1,self.k])\n",
        "    token_emb = self.token_emb(x)\n",
        "    token_emb = torch.cat((cls_token, token_emb), dim=1)\n",
        "\n",
        "    pos_emb = torch.arange(t+1, device=x.device)\n",
        "    pos_emb = self.pos_emb(pos_emb).unsqueeze(0).expand([b,t+1,self.k])\n",
        "\n",
        "    emb = token_emb + pos_emb\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    out = out[:,0]\n",
        "\n",
        "    return self.ff(out)"
      ],
      "metadata": {
        "id": "RPD7Xaq0AV_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTTransformer(nn.Module):\n",
        "  def __init__(self, k, head, depth, patch_size, img_size, channel, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0\n",
        "\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.patch_dim = patch_size * patch_size * channel\n",
        "    self.num_patch = (img_size // patch_size) ** 2\n",
        "    self.token_emb = nn.Linear(self.patch_dim, k)\n",
        "    self.pos_emb = nn.Parameter(torch.rand(1,self.num_patch+1,k))\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,k))\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(depth):\n",
        "      blocks.append(TransformerBlock(k, head))\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.size()\n",
        "    p = self.pathch_size\n",
        "\n",
        "    x = rearrange(x, \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\",\n",
        "            p1=p, p2=p)\n",
        "\n",
        "    token_emb = self.token_emb(x)\n",
        "\n",
        "    cls_token = repeat(self.cls_token, \"1 1 k -> b 1 k\", b=B)\n",
        "    token_emb = torch.cat((cls_token, token_emb), dim=1)\n",
        "\n",
        "    emb = token_emb + self.pos_emb[:,:token_emb.size(1)]\n",
        "\n",
        "    out = self.blocks(emb)\n",
        "\n",
        "    cls_out = out[:, 0]\n",
        "\n",
        "    return self.ff(cls_out)"
      ],
      "metadata": {
        "id": "3DwaZ-pHvisf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.pos_emb = nn.Embedding(seq_len, k)\n",
        "\n",
        "    self.block = nn.Sequential(*[TransformerBlock(k, head) for _ in range(depth)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    token = self.token_emb(x)\n",
        "    pos = self.pos_emb(torch.arange(token.size(1), device=x.device)).unsqueeze(0)\n",
        "\n",
        "    emb = token+pos\n",
        "\n",
        "    return self.block(emb)"
      ],
      "metadata": {
        "id": "q5yucV_lxuaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "你不能使用 nn.Sequential([...])，因為每一層 TransformerBlock 需要接受兩個參數：\n",
        "\n",
        "```python\n",
        "x = block(x, enc_out)\n",
        "```\n",
        "但 nn.Sequential 預設只會把 上一層的輸出傳給下一層作為唯一參數。無法處理 cross_attention=True 的 TransformerBlock 所需的額外 enc_out。"
      ],
      "metadata": {
        "id": "CdDWR1Kib28c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.head = head\n",
        "\n",
        "    self.token_emb = nn.Embedding(token_size, k)\n",
        "    self.cls_token = nn.Parameter(torch.rand(1,1,k))\n",
        "    self.pos_emb = nn.Embedding(seq_len+1, k)\n",
        "\n",
        "    self.block = nn.ModuleList([TransformerBlock(k, head, True) for _ in range(depth)])\n",
        "\n",
        "    for b in self.block:\n",
        "      b.self_attn.mask = True\n",
        "\n",
        "    self.ff = nn.Linear(k, num_class)\n",
        "\n",
        "  def forward(self, x, enc_out):\n",
        "    cls_token = repeat(self.cls_token, \"1 1 k -> b 1 k\", b=x.size(0))\n",
        "    token = self.token_emb(x)\n",
        "    token = torch.cat((cls_token, token), dim=1)\n",
        "\n",
        "    pos = self.pos_emb(torch.arange(token.size(1), device=x.device)).unsqueeze(0)\n",
        "\n",
        "    emb = token+pos\n",
        "\n",
        "    for b in self.block:\n",
        "      emb = b(emb, enc_out)\n",
        "\n",
        "    return self.ff(emb[:,0])"
      ],
      "metadata": {
        "id": "MfFpKsqETsx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTransformerEncoderDecoder(nn.Module):\n",
        "  def __init__(self, k, head, depth, seq_len, token_size, num_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(k, head, depth, seq_len, token_size)\n",
        "    self.decoder = Decoder(k, head, depth, seq_len, token_size, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    enc_out = self.encoder(x)\n",
        "    return self.decoder(x, enc_out)"
      ],
      "metadata": {
        "id": "Q54NJqdLcPCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}